ğŸš¨ Why I Built This Project: Understanding Real Cloud Outages
Simulating How AWS, Cloudflare, and Azure Outages Happen â€” And How Cloud Engineers Fix Them

Over the past year, several major cloud providers â€” AWS, Cloudflare, Azure, Microsoft â€” experienced large-scale outages that took down hundreds of apps and businesses.

I kept asking myself:

â€œWhy do these outages happen? And how do cloud / SRE engineers fix them in real time?â€

Instead of Googling itâ€¦
I built a full serverless microservices system and intentionally broke it, then diagnosed and fixed the outage like a real cloud engineer.

This project is the result.


ğŸš€ AWS Serverless Microservices â€” Outage Simulation & Recovery

A production-style, 3-service microservices architecture built using:

AWS Lambda

API Gateway (HTTP API)

CloudWatch Metrics & Logs

Python

Includes a realistic outage simulation, debugging workflow, and recovery process â€” the exact steps cloud engineers follow during real incidents.


ğŸ§± Architecture Overview

This system uses three independent microservices, each running as its own Lambda function:

1ï¸âƒ£ Auth Service

Simple health check

Mimics authentication systems in real apps

2ï¸âƒ£ Product Service

Returns mock products

Represents catalog / inventory microservices

3ï¸âƒ£ Payment Service (most important)

Broken Version â†’ throws errors (simulated outage)

Fixed Version â†’ returns healthy JSON

Demonstrates realistic dependency failure & recovery

These services are wired through API Gateway, just like real production APIs.

ğŸ’¥ Realistic Cloud Outage Simulation

To replicate how real outages happen (like the AWS / Cloudflare ones), I forced the payment service to fail:
raise Exception("Simulated payment service failure")

This created a chain of events identical to real incidents:
âŒ API Gateway returned 500 errors
ğŸ“‰ CloudWatch error metrics spiked
ğŸ” Logs captured stack traces
ğŸ›‘ Payment flow became unavailable

This is EXACTLY how outages start in real microservice-based environments.

ğŸ”§ Diagnosis & Recovery (Real Cloud Engineering Process)

After simulating the outage, I:

1. Investigated CloudWatch Metrics

Saw error count increase â†’ confirmed the failure.

2. Checked CloudWatch Logs

Found the forced exception â†’ root cause identified.

3. Redeployed a fixed version of the payment service:
{
  "service": "payment-service",
  "status": "ok",
  "message": "payment service reachable",
  "timestamp": 123456
}

4. Validated the fix

Errors dropped

Invocations succeeded

Service returned to green

This entire cycle â€” break â†’ detect â†’ diagnose â†’ fix â†’ validate â€” mirrors real SRE / CloudOps workflows

ğŸŒ API Endpoints
Method	Route	Service
GET	/auth	Auth Service
GET	/product	Product Service
GET	/pay	Payment Service (Broken)
GET	/pay-fixed	Payment Service (Healthy)

ğŸ“¸ Screenshots Included (Full Walkthrough)

Inside the /screenshots folder, youâ€™ll find:

API Gateway configuration

Lambda functions list

Working /auth and /product responses

Broken payment service (500 error)

Fixed payment service

CloudWatch error spike during outage

CloudWatch recovery after fix

These screenshots tell the full story of the outage and recovery.

ğŸ¯ Final Thoughts

This project wasnâ€™t just about writing code â€” it was a real experiment in understanding how cloud outages happen at companies like AWS, Cloudflare, and Azure.

By intentionally breaking a microservice, tracing the failure, analyzing metrics, and restoring the system, I recreated the exact workflow cloud and security engineers follow during real incidents.

This project demonstrates curiosity, real engineering thinking, and hands-on AWS experience â€” not just theory.

It reflects how I approach problems:
break it, analyze it, understand it, fix it, and make it better.
