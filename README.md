ğŸš¨ Why I Built This Project
Understanding Real Cloud Outages â€” and Recreating How Cloud Engineers Diagnose & Fix Them

For years weâ€™ve seen major outages hit platforms like AWS and Microsoft Azure, bringing down authentication systems, APIs, applications, and entire services around the world.

Those outages always made me wonder:

â€œWhat actually causes critical cloud services to go down â€” and how do Cloud, SRE, and Security Engineers bring them back online?â€

I didnâ€™t want the theoretical answer.
I wanted the engineering answer.

So instead of reading articles,
I built a real microservices architecture, intentionally broke part of it, and then diagnosed & restored the system exactly like cloud engineers do in production.

This project is the result.

ğŸš€ AWS Serverless Microservices â€” Outage Simulation & Recovery

A production-style microservices environment built using:

AWS Lambda

API Gateway (HTTP API)

CloudWatch Metrics & Logs

Python (serverless runtime)

It includes a simulated service outage, real failure signals, monitoring analysis, and a full debugging + recovery workflow â€” the same approach used during real AWS/Azure incident response.

ğŸ§± Architecture Overview

The architecture consists of three isolated microservices, each deployed as an independent AWS Lambda function:

1ï¸âƒ£ Auth Service

Basic health check

Represents authentication/identity components

2ï¸âƒ£ Product Service

Returns mock product data

Mimics catalog or inventory microservices

3ï¸âƒ£ Payment Service (The Core of the Simulation)

Broken Version â†’ intentionally throws an exception (simulating failure)

Fixed Version â†’ returns a healthy JSON response

Demonstrates dependency failure, availability issues, and safe restoration

All services are exposed via API Gateway, similar to how distributed systems expose public/internal APIs in AWS and Azure.

ğŸ’¥ Realistic Cloud Outage Simulation

To replicate the types of cascading failures seen on AWS and Azure, I intentionally forced the payment service to fail by raising an exception:

raise Exception("Simulated payment service failure")


This instantly triggered failure patterns identical to those in real cloud outages:

âŒ API Gateway returned 500 Internal Server Error
ğŸ“‰ CloudWatch error rate increased
ğŸ” Logs revealed stack traces & root cause clues
ğŸ›‘ The payment workflow became unavailable

This mirrors how a single failing microservice in AWS/Azure can cause system-wide degradation.

ğŸ”§ Diagnosis & Recovery â€” Real SRE Workflow

After triggering the outage, I followed an SRE-style incident response cycle:

1ï¸âƒ£ Investigate Metrics

Checked CloudWatch Metrics

Verified spikes in errors & failed invocations

2ï¸âƒ£ Analyze Logs

Located the exception

Confirmed the exact failure point

3ï¸âƒ£ Deploy Fix

Replaced the failing logic with a healthy response:

{
  "service": "payment-service",
  "status": "ok",
  "message": "payment service reachable",
  "timestamp": 123456
}

4ï¸âƒ£ Validate the System

Errors dropped

Success rate returned to normal

API Gateway served healthy responses

This break â†’ detect â†’ diagnose â†’ fix â†’ validate cycle is exactly how AWS and Azure engineers manage incidents.

ğŸŒ API Endpoints
Method	Route	Service
GET	/auth	Auth Service
GET	/product	Product Service
GET	/pay	Payment (Broken)
GET	/pay-fixed	Payment (Healthy)
ğŸ“¸ Full Walkthrough (Screenshots)

Inside the /screenshots folder, youâ€™ll find:

API Gateway configuration

Lambda functions list

Healthy /auth response

Healthy /product response

Broken /pay response (500)

Fixed /pay-fixed response

CloudWatch error spike during outage

CloudWatch recovery after the fix

These screenshots document the entire engineering process.

â­ What This Project Demonstrates
Cloud Engineering Skills

Serverless architecture

Microservice separation

API Gateway routing

AWS operational understanding

Environment isolation

SRE / CloudOps Skills

Observability (metrics, logs, failure signals)

Incident triage & root cause analysis

Failure injection (fault simulation)

Safe recovery workflows

Post-incident validation

Cybersecurity Relevance

Availability (core of the CIA Triad)

Dependency hardening

Service health monitoring

Understanding how failure impacts resilience

Cloud-native security awareness

This project mirrors the exact workflow used by AWS, Azure, FinTech, and SaaS engineering teams during real cloud failures.

ğŸ¯ Final Thoughts

This wasnâ€™t just a coding exercise â€”
it was an experiment in understanding failure, resilience, and incident response in cloud environments.

By intentionally breaking a microservice, analyzing the blast radius, interpreting metrics, and restoring system health, I recreated the real-world engineering process behind major outages on AWS and Azure.

This project reflects my approach to engineering:

Understand it.
Break it.
Observe it.
Fix it.
Improve it.
