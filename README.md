ğŸš¨ Why I Built This Project

*Understanding Real Cloud Outages â€” and Recreating How Cloud Engineers Diagnose & Fix Them*

For years, major cloud platforms like AWS and Microsoft Azure have had outages that disrupted thousands of apps and businesses.

Those outages made me wonder:

â€œWhat actually causes critical cloud services to go down â€” and how do Cloud, SRE, and Security Engineers bring them back online?â€

I didnâ€™t want a theoretical explanation â€” I wanted the real engineering experience.

So I built a full serverless microservices system, intentionally broke part of it, then diagnosed and restored it the same way real cloud engineers handle production incidents.

This project is the result.

---

ğŸš€ AWS Serverless Microservices â€” Outage Simulation & Recovery

A production-style microservices environment built using:

- AWS Lambda

- API Gateway (HTTP API)

- CloudWatch Metrics & Logs

- Python

This project includes:

- A simulated microservice outage

- Real failure signals (500 errors, CloudWatch spikes, logs)

- A full debugging workflow

- Complete service recovery

- Validation of system health

Exactly how AWS/Azure engineers operate during real incidents.

---

ğŸ§± Architecture Overview

This system consists of three isolated microservices, each deployed as its own AWS Lambda function:

---

1ï¸âƒ£ Auth Service

- Returns a basic health check

- Represents authentication/identity components

---

2ï¸âƒ£ Product Service

- Returns mock product data

- Mimics catalog or inventory microservices

---

3ï¸âƒ£ Payment Service (Core of the Simulation)

- Broken Version â†’ intentionally throws an exception

- Fixed Version â†’ returns a healthy JSON response

- Demonstrates dependency failures, outages, and recovery

---

All three services are routed through API Gateway, similar to real-world distributed systems in AWS and Azure.

---



ğŸ’¥ Realistic Cloud Outage Simulation

To recreate the type of cascading failures seen in cloud outages, I deliberately forced the payment service to fail:

raise Exception("Simulated payment service failure")

This triggered the same patterns seen in real cloud incidents:

- âŒ API Gateway returned 500 Internal Server Errors

- ğŸ“‰ CloudWatch error metrics spiked

- ğŸ” Logs captured stack traces & failure signatures

- ğŸ›‘ The payment workflow became unavailable

This mirrors how a single failing microservice can impact downstream systems.

---

ğŸ”§ Diagnosis & Recovery â€” Real SRE Workflow

After triggering the outage, I followed an industry-style incident response cycle:



1ï¸âƒ£ Investigate Metrics

- Checked CloudWatch Metrics

- Confirmed spikes in errors and failed invocations



2ï¸âƒ£ Analyze Logs

- Located the exception

- Verified the root cause



3ï¸âƒ£ Deploy Healthy Version

Replaced failing logic with a fixed response:

{
  "service": 
  "payment-service",
   "status": "ok",
  "message": "payment service reachable",
  "timestamp": 123456
}




4ï¸âƒ£ Validate the Fix

- Error rate dropped

- Success rate returned to normal

- API Gateway responded with healthy results again



This break â†’ detect â†’ diagnose â†’ fix â†’ validate workflow is the same pattern used by CloudOps and SRE teams at AWS/Azure.

---


ğŸŒ API Endpoints:


- GET	/auth	Auth Service
---
- GET	/product	Product Service
---
- GET	/pay	Payment (Broken)
---
- GET	/pay-fixed	Payment (Healthy)

---

ğŸ“¸ Screenshot Walkthrough

The /screenshots folder includes:

- API Gateway configuration

- Lambda functions overview

- Healthy /auth response

- Healthy /product response

- Broken /pay response (500 error)

- Fixed /pay-fixed response

- CloudWatch error spike

- CloudWatch recovery after the fix

These images show the full outage â†’ diagnosis â†’ recovery lifecycle.

---

â­ What This Project Demonstrates
Cloud Engineering

- Serverless architecture

- API Gateway routing

- Isolated microservices

- AWS operational knowledge

SRE / CloudOps

- Fault injection

- Incident analysis

- Observability (logs, metrics, error rates)

- Root cause identification

- Progressive recovery

Cybersecurity (Availability)

- Service resilience

- Failure impact analysis

- High-availability concepts

- Recovery validation

ğŸ¯ Final Thoughts

This wasnâ€™t just a coding project â€”
it was an experiment in understanding how real cloud outages happen and how engineers bring systems back online.

By intentionally breaking a microservice, analyzing the blast radius, examining CloudWatch data, and restoring service health, I recreated the workflows used by real Cloud, DevOps, Security, and SRE teams.

This project reflects my engineering mindset:

Understand it â†’ Break it â†’ Observe it â†’ Fix it â†’ Improve it.
